"""
SCRAPER COMPLET - EMPLOI DU TEMPS 6 MOIS (+ ARCHIVAGE HTML)
===========================================================
Scrape 26 semaines (6 mois) d'emploi du temps
Stocke les Ã©vÃ©nements en JSON et archive le HTML brut hebdomadaire
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime, timedelta
import re
import os
import sys

# Importer les fonctions du scraper original (si nÃ©cessaire pour deps externes)
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

class CelcatCompleteScraper:
    def __init__(self, login_url, username, password):
        """
        Scraper complet pour 6 mois d'emploi du temps
        
        Args:
            login_url: URL de la page de connexion
            username: Identifiant
            password: Mot de passe
        """
        self.login_url = login_url
        self.username = username
        self.password = password
        self.driver = None
        self.all_events = []
        
        # Configuration de l'archivage
        self.archive_dir = "archives_html"
        self._setup_archive_dir()
        
    def setup_driver(self, headless=True):
        """Configure le navigateur Chrome"""
        print("ğŸ”§ Configuration du navigateur...")
        
        options = webdriver.ChromeOptions()
        
        if headless:
            options.add_argument('--headless')
        
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('--window-size=1920,1080')
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=options)
        
        print("âœ… Navigateur prÃªt!")
        return self.driver
    
    def login(self):
        """Se connecte Ã  Celcat"""
        print(f"\nğŸ” Connexion Ã  {self.login_url}...")
        
        try:
            self.driver.get(self.login_url)
            time.sleep(8)
            
            # Recherche des champs de connexion
            try:
                username_field = self.driver.find_element(By.NAME, "username")
                password_field = self.driver.find_element(By.NAME, "password")
            except:
                try:
                    username_field = self.driver.find_element(By.CSS_SELECTOR, "input[type='text']")
                    password_field = self.driver.find_element(By.CSS_SELECTOR, "input[type='password']")
                except:
                    username_field = self.driver.find_element(By.ID, "username")
                    password_field = self.driver.find_element(By.ID, "password")
            
            username_field.clear()
            username_field.send_keys(self.username)
            password_field.clear()
            password_field.send_keys(self.password)
            
            print("âœ… Identifiants saisis")
            
            # Recherche du bouton de connexion
            submit_selectors = [
                "button[type='submit']",
                "input[type='submit']",
                "//button[contains(text(), 'Connexion')]",
                "//button[contains(text(), 'Se connecter')]"
            ]
            
            submit_button = None
            for selector in submit_selectors:
                try:
                    if selector.startswith("//"):
                        submit_button = self.driver.find_element(By.XPATH, selector)
                    else:
                        submit_button = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if submit_button:
                        break
                except:
                    continue
            
            if submit_button:
                submit_button.click()
            else:
                username_field.submit()
            
            time.sleep(8)
            
            print("âœ… Connexion rÃ©ussie!")
            return True
            
        except Exception as e:
            print(f"âŒ Erreur lors de la connexion: {e}")
            return False
    
    def navigate_to_week(self, week_date):
        """Navigate vers une semaine spÃ©cifique"""
        print(f"\nğŸ“… Navigation vers la semaine du {week_date.strftime('%d/%m/%Y')}...")
        
        try:
            # Cliquer sur le sÃ©lecteur de date
            date_picker = self.driver.find_element(By.CLASS_NAME, "fc-center")
            date_picker.click()
            time.sleep(2)
            
            # Chercher et cliquer sur la date
            date_cells = self.driver.find_elements(By.CSS_SELECTOR, "td[data-date]")
            
            target_iso = week_date.strftime("%Y-%m-%d")
            found = False
            
            for cell in date_cells:
                cell_date = cell.get_attribute("data-date")
                if cell_date == target_iso:
                    cell.click()
                    time.sleep(3)
                    found = True
                    break
            
            if not found:
                # Fallback si la date n'est pas dans le mois affichÃ©
                print("   âš ï¸ Date non visible directement, utilisation navigation bouton...")
            
            return True
            
        except Exception as e:
            print(f"âš ï¸  Navigation manuelle nÃ©cessaire: {e}")
            return True
    
    def save_week_html(self, week_date):
        """
        Sauvegarde le HTML brut de la semaine courante
        """
        try:
            filename = f"week_{week_date.strftime('%Y-%m-%d')}.html"
            filepath = os.path.join(self.archive_dir, filename)
            
            html_content = self.driver.page_source
            
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(html_content)
                
            print(f"   ğŸ’¾ HTML archivÃ©: {filename}")
            return True
        except Exception as e:
            print(f"   âŒ Erreur archivage HTML: {e}")
            return False

    def extract_week_events(self, week_date):
        """Extrait les Ã©vÃ©nements de la semaine courante"""
        print(f"ğŸ” Extraction des Ã©vÃ©nements de la semaine {week_date.strftime('%d/%m/%Y')}...")
        
        page_source = self.driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        
        events = []
        
        # Extraire les dates de la semaine
        dates_map = self.extract_week_dates(soup)
        
        if not dates_map:
            print("âŒ Impossible d'extraire les dates")
            return events
        
        # Extraire les Ã©vÃ©nements
        content_skeleton = soup.find('div', class_='fc-content-skeleton')
        
        if not content_skeleton:
            print("âš ï¸  Aucun Ã©vÃ©nement trouvÃ© pour cette semaine")
            return events
        
        event_containers = content_skeleton.find_all('a', class_='fc-time-grid-event')
        
        print(f"   ğŸ“Œ {len(event_containers)} Ã©vÃ©nements trouvÃ©s")
        
        for event_tag in event_containers:
            try:
                event = self.parse_event(event_tag, dates_map)
                if event and event not in events:
                    events.append(event)
            except Exception as e:
                print(f"   âš ï¸  Erreur parsing Ã©vÃ©nement: {e}")
                continue
        
        return events
    
    def extract_week_dates(self, soup):
        """Extrait les dates des colonnes de la semaine"""
        dates_map = {}
        try:
            day_headers = soup.find_all('th', class_='fc-day-header')
            for header in day_headers:
                date_str = header.get('data-date')
                if date_str:
                    classes = header.get('class', [])
                    day_classes = ['fc-sun', 'fc-mon', 'fc-tue', 'fc-wed', 'fc-thu', 'fc-fri', 'fc-sat']
                    col_idx = 0
                    for idx, day_class in enumerate(day_classes):
                        if day_class in classes:
                            col_idx = idx
                            break
                    dates_map[col_idx] = date_str
        except Exception as e:
            print(f"Erreur extraction dates: {e}")
        return dates_map
    
    def parse_event(self, event_tag, dates_map):
        """Parse un Ã©vÃ©nement individuel"""
        event = {
            'date': '', 'start_time': '', 'end_time': '', 'title': '',
            'course_code': '', 'course_name': '', 'location': '',
            'teacher': '', 'type': '', 'groups': []
        }
        
        try:
            # Date
            style = event_tag.get('style', '')
            left_match = re.search(r'left:\s*(\d+(?:\.\d+)?)%', style)
            if left_match:
                left_percent = float(left_match.group(1))
                col_idx = int(left_percent / 14.2857)
                event['date'] = dates_map.get(col_idx, '')
            
            # Heures
            time_span = event_tag.find('span', class_='fc-time')
            if time_span:
                time_text = time_span.get('data-full', '') or time_span.text
                time_match = re.match(r'(\d{2}:\d{2})\s*-\s*(\d{2}:\d{2})', time_text)
                if time_match:
                    event['start_time'] = time_match.group(1)
                    event['end_time'] = time_match.group(2)
            
            # Contenu
            fc_content = event_tag.find('div', class_='fc-content')
            if fc_content:
                title_span = fc_content.find('span', class_='fc-title')
                if title_span:
                    full_text = title_span.get_text(separator=' ', strip=True)
                    parts = [p.strip() for p in full_text.split('\n') if p.strip()]
                    
                    if parts:
                        code_match = re.match(r'^(0?\d{1,3})', parts[0])
                        if code_match:
                            event['course_code'] = code_match.group(1)
                            parts[0] = parts[0][len(event['course_code']):].strip()
                    
                    if parts and parts[0]: event['title'] = parts[0]
                    
                    for i, part in enumerate(parts):
                        if len(part) > 15 and not any(kw in part for kw in ['TD', 'TP', 'CM', 'Amphi']):
                            event['course_name'] = part
                            parts.pop(i)
                            break
                    
                    loc_keywords = ['Amphi', 'TD', 'TP', 'Salle', 'Porte', 'Espace', 'e-learning']
                    for part in parts:
                        if any(kw in part for kw in loc_keywords):
                            event['location'] = part
                            break
                    
                    for part in parts:
                        if re.match(r'^[A-ZÃ€-Ã][a-zÃ Ã¨Ã©ÃªÃ«Ã¯Ã®Ã´Ã¹Ã»Ã§\-]+\s+[A-ZÃ€-Ã]', part):
                            event['teacher'] = part
                            break
                    
                    type_keywords = ['CM', 'TD', 'TP', 'Examen', 'Evaluation']
                    for part in parts:
                        if part.strip() in type_keywords:
                            event['type'] = part.strip()
                            break
                    
                    for part in parts:
                        if 'VET' in part or 'classe' in part:
                            clean_group = part.split('[')[0].strip() if '[' in part else part.strip()
                            if clean_group and clean_group not in event['groups']:
                                event['groups'].append(clean_group)
        
        except Exception as e:
            print(f"Erreur parsing: {e}")
        
        return event
    
    def scrape_full_semester(self, nb_weeks=26):
        """
        Scrape nb_weeks semaines Ã  partir d'aujourd'hui et archive le HTML
        """
        print(f"\n{'='*70}")
        print(f"ğŸ“ SCRAPING COMPLET - {nb_weeks} SEMAINES + ARCHIVAGE")
        print(f"{'='*70}\n")
        
        start_date = datetime.now()
        
        for week_num in range(nb_weeks):
            try:
                # Calculer la date de la semaine
                week_date = start_date + timedelta(weeks=week_num)
                
                print(f"\nğŸ“… Semaine {week_num + 1}/{nb_weeks} - {week_date.strftime('%d/%m/%Y')}")
                print("-" * 50)
                
                # Navigation
                if week_num > 0:
                    try:
                        next_button = self.driver.find_element(By.CLASS_NAME, "fc-next-button")
                        next_button.click()
                        time.sleep(3) # Attente chargement AJAX
                    except Exception as e:
                        print(f"âš ï¸  Impossible de naviguer: {e}")
                
                # --- NOUVEAUTÃ‰ : Archivage HTML ---
                self.save_week_html(week_date)
                # ----------------------------------
                
                # Extraction
                week_events = self.extract_week_events(week_date)
                
                for event in week_events:
                    if event not in self.all_events:
                        self.all_events.append(event)
                
                print(f"âœ… {len(week_events)} Ã©vÃ©nements extraits")
                print(f"ğŸ“Š Total actuel: {len(self.all_events)} Ã©vÃ©nements")
                
                time.sleep(2)
                
            except Exception as e:
                print(f"âŒ Erreur semaine {week_num + 1}: {e}")
                continue
        
        print(f"\n{'='*70}")
        print(f"âœ… SCRAPING TERMINÃ‰")
        print(f"ğŸ“Š Total final: {len(self.all_events)} Ã©vÃ©nements")
        print(f"ğŸ“ Archives HTML disponibles dans : {self.archive_dir}/")
        print(f"{'='*70}\n")
    
    def save_events(self, filename='emploi_du_temps_complet.json'):
        """Sauvegarde tous les Ã©vÃ©nements dans un fichier JSON"""
        if not self.all_events:
            print("âš ï¸  Aucun Ã©vÃ©nement Ã  sauvegarder")
            return
        
        self.all_events.sort(key=lambda x: (x['date'], x['start_time']))
        
        output = {
            'metadata': {
                'scrape_date': datetime.now().isoformat(),
                'total_events': len(self.all_events),
                'archive_dir': self.archive_dir
            },
            'events': self.all_events
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… JSON sauvegardÃ© dans {filename}")
    
    def close(self):
        """Ferme le navigateur"""
        if self.driver:
            self.driver.quit()
            print("ğŸ”’ Navigateur fermÃ©")


def main():
    """Fonction principale"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                          â•‘
    â•‘   ğŸ“ SCRAPER COMPLET - 6 MOIS (26 SEMAINES) + ARCHIVE ğŸ“  â•‘
    â•‘                                                          â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    try:
        with open('reponse.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
        print("âœ… Configuration chargÃ©e depuis reponse.json")
    except FileNotFoundError:
        print("âŒ Erreur : Le fichier 'reponse.json' est introuvable.")
        return
    
    LOGIN_URL = config.get("login_url", "").strip()
    USERNAME = config.get("username", "").strip()
    PASSWORD = config.get("password", "").strip()
    
    NB_WEEKS = 26
    
    scraper = CelcatCompleteScraper(LOGIN_URL, USERNAME, PASSWORD)
    
    try:
        scraper.setup_driver(headless=True)  # HEADLESS pour GitHub Actions
        
        if not scraper.login():
            print("\nâŒ Ã‰chec de la connexion")
            return
        
        time.sleep(5)
        
        print("\nğŸ”„ Passage en vue hebdomadaire...")
        try:
            week_button = scraper.driver.find_element(By.CSS_SELECTOR, "button.fc-agendaWeek-button")
            week_button.click()
            time.sleep(3)
        except:
            pass
        
        scraper.scrape_full_semester(nb_weeks=NB_WEEKS)
        scraper.save_events('emploi_du_temps_complet.json')
        
    except Exception as e:
        print(f"\nâŒ Erreur: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # Input supprimÃ© pour GitHub Actions
        # input("\nâ¸ï¸  Appuyez sur EntrÃ©e pour fermer...")
        scraper.close()

if __name__ == "__main__":
    main()
